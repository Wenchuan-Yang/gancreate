{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkBXmZVECvJk"
   },
   "source": [
    "# API Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "Y2dxr6ZhC2D6",
    "outputId": "d1de2355-f8e9-497e-cfb7-fbdfe33d040e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install ninja opensimplex gradio moviepy==1.0.3 tts pyrebase\n",
    "# !apt install ffmpeg\n",
    "# !pip install git+https://github.com/1adrianb/face-alignment@v1.0.1\n",
    "# !pip install fbpca boto3 requests==2.23.0 #urllib3==1.25.11\n",
    "# !git submodule update --init --recursive\n",
    "\n",
    "# !python -c \"import nltk; nltk.download('wordnet')\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在nltk中集成了语料与模型等的包管理器，通过在python解释器中执行\n",
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jib1bUT4C6Cr",
    "outputId": "5d3932a3-cf67-4d97-a631-2085cd59639f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### colab上运行jupyter notebook\n",
    "\n",
    "# from google.colab import drive\n",
    "# '''\n",
    "# Colab全称Colaboratory，是Google提供的一个Jupyter Notebook式的交互环境，\n",
    "# 在云端运行，提供免费的GPU资源，用户可以直接使用。\n",
    "# 并且创建的Notebook存储在云端硬盘中，可以方便与他人共享。\n",
    "# '''\n",
    "\n",
    "# drive.mount('/content/drive')   \n",
    "# %cd /content/drive/MyDrive/Colab/fyp\n",
    "\n",
    "# try:   # set up path\n",
    "#     import sys\n",
    "#     sys.path.append('/content/drive/MyDrive/Colab/fyp/ganspace')\n",
    "#     sys.path.append('/content/drive/MyDrive/Colab/fyp/first-order-model')\n",
    "#     sys.path.append('/content/drive/MyDrive/Colab/fyp/iPERCore')\n",
    "#     sys.path.append('/content/drive/MyDrive/Colab/fyp/Wav2Lip')\n",
    "#     print('Paths added')\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "04KOp1ofD3SG",
    "outputId": "7acf1f21-7561-4951-b55d-868c5c6d4a6b"
   },
   "outputs": [],
   "source": [
    "#@title Install impersonator dependencies\n",
    "# set CUDA_HOME, here we use CUDA 10.1\n",
    "\n",
    "# from IPython.display import Javascript\n",
    "# display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 200})'''))\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-10.1\"\n",
    "\n",
    "# !echo $CUDA_HOME\n",
    "# %cd /content/drive/MyDrive/Colab/fyp/iPERCore/\n",
    "# !python setup.py develop\n",
    "# %cd /content/\n",
    "\n",
    "\n",
    "# 添加代码\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# \"\"\"\n",
    "# 1、os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"#按照PCI_BUS_ID顺序从0开始排列GPU设备\n",
    "# 2、os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"#设置当前使用的GPU设备仅为0号设备 设备名称为‘/gpu:0'\n",
    "# 3、os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"#设置当前使用的GPU设备仅为1号设备 设备名称为'/gpu:0'\n",
    "# 4、os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"#设置当前的GPU设备为0，1号两个设备，名称依次为’/gpu:0'，'/gpu:1'。表示优先使用0号设备，然后使用1号设备。\n",
    "# 5、os.environ[\"CUDA_VISIBLE_DEVICES\"=\"-1\"#设置当前的设备使用CPU\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135,
     "referenced_widgets": [
      "b8d2defd6a4e4f56a96a9dab7718aac7",
      "e523c3f102f14a6fa60cd5b74c4b48ce",
      "3a23ea6464e643d5ad642fd2ac9864b6",
      "9b3f973175a44a96b28f01f3b2be5ee6",
      "8e171faf256a481aafac35d0416f5001",
      "967eb2d314f44aec89a0d1c6bed078f6",
      "40111ff076384a9297ead2f816aff1cb",
      "90c4a4ed8f9e4343b5ff7dc507342829",
      "9fe6e45e0c2b4542800ce9ff6251dc94",
      "767b54422b024a958e28adc0604c8326",
      "8bd2eef682944619ab06d381d2dc390d",
      "a4539b483292426dadd6aa0d0f74ac38",
      "28ce95a28cf04278999daed8a099a2c5",
      "8be477635be44e9f87dff8af41017553",
      "ff6feac3822e4bcea52220ee2f00d8cd",
      "a3e566c81fe642479a5c29a01f96cedc",
      "57280342733a429fa2d327788d8c1a87",
      "8b07dded0028443e86a65b02c9afcfdf",
      "ced67228d3bd477bab466e8e1c4ae3ef",
      "394d9c1390f1484f82d47899bbabbde9",
      "a1453a0cf1f94168993482ffe76fa721",
      "e1047a5345a34e028d4ba5a2f38f2c31"
     ]
    },
    "id": "CnsBkNM4DC3K",
    "outputId": "91b82e67-a4b1-4580-9697-68636a59a0ee",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m hog \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mHOGDescriptor() \u001b[38;5;66;03m# 可以直接获取图片的HOG特征\u001b[39;00m\n\u001b[1;32m     18\u001b[0m hog\u001b[38;5;241m.\u001b[39msetSVMDetector(cv2\u001b[38;5;241m.\u001b[39mHOGDescriptor_getDefaultPeopleDetector())\n\u001b[0;32m---> 20\u001b[0m fa \u001b[38;5;241m=\u001b[39m \u001b[43mface_alignment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceAlignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_alignment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLandmarksType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_2D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflip_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 检测人脸的2D坐标\u001b[39;00m\n\u001b[1;32m     22\u001b[0m image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 创建边界框\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/face_alignment/api.py:69\u001b[0m, in \u001b[0;36mFaceAlignment.__init__\u001b[0;34m(self, landmarks_type, network_size, device, flip_input, face_detector, verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Get the face detector\u001b[39;00m\n\u001b[1;32m     67\u001b[0m face_detector_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface_alignment.detection.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m face_detector,\n\u001b[1;32m     68\u001b[0m                                   \u001b[38;5;28mglobals\u001b[39m(), \u001b[38;5;28mlocals\u001b[39m(), [face_detector], \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_detector \u001b[38;5;241m=\u001b[39m \u001b[43mface_detector_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Initialise the face alignemnt networks\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_alignment_net \u001b[38;5;241m=\u001b[39m FAN(network_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/face_alignment/detection/sfd/sfd_detector.py:28\u001b[0m, in \u001b[0;36mSFDDetector.__init__\u001b[0;34m(self, device, path_to_detector, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_detector \u001b[38;5;241m=\u001b[39m s3fd()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_detector\u001b[38;5;241m.\u001b[39mload_state_dict(model_weights)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_detector\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/torch/nn/modules/module.py:673\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    670\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/torch/nn/modules/module.py:387\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 387\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    391\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    398\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/torch/nn/modules/module.py:409\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 409\u001b[0m         param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/torch/nn/modules/module.py:671\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    670\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/torch/cuda/__init__.py:170\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    174\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "#@title Define Cropping Functions\n",
    "#  数据的裁剪\n",
    "import os\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import face_alignment # 人脸对齐\n",
    "import imageio # 提供了一个易于阅读和 编写广泛的图像数据，包括动画图像、体积 数据和科学格式。\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, clear_output\n",
    "import cv2\n",
    "import shutil # shutil模块是对os模块的补充，主要针对文件的拷贝、删除、移动、压缩和解压操作。\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "hog = cv2.HOGDescriptor() # 可以直接获取图片的HOG特征\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True, device='cuda') # 检测人脸的2D坐标\n",
    "\n",
    "image_size = 512\n",
    "\n",
    "# 创建边界框\n",
    "def create_bounding_box(target_landmarks, expansion_factor=1):\n",
    "    target_landmarks = np.array(target_landmarks)\n",
    "    x_y_min = target_landmarks.reshape(-1, 68, 2).min(axis=1)\n",
    "    x_y_max = target_landmarks.reshape(-1, 68, 2).max(axis=1)\n",
    "    expansion_factor = (expansion_factor-1)/2\n",
    "    bb_expansion_x = (x_y_max[:, 0] - x_y_min[:, 0]) * expansion_factor\n",
    "    bb_expansion_y = (x_y_max[:, 1] - x_y_min[:, 1]) * expansion_factor\n",
    "    x_y_min[:, 0] -= bb_expansion_x\n",
    "    x_y_max[:, 0] += bb_expansion_x\n",
    "    x_y_min[:, 1] -= bb_expansion_y\n",
    "    x_y_max[:, 1] += bb_expansion_y\n",
    "    return np.hstack((x_y_min, x_y_max-x_y_min))\n",
    "\n",
    "# 固定数组维度\n",
    "def fix_dims(im):\n",
    "    if im.ndim == 2: #  .dims：返回的是数组的维度\n",
    "        im = np.tile(im[..., None], [1, 1, 3]) # np.tile（a,(2)）函数的作用就是将函数沿着X轴扩大两倍\n",
    "    return im[...,:3]\n",
    "\n",
    "\n",
    "def get_crop(im, center_face=True, crop_face=True, expansion_factor=1, landmarks=None):\n",
    "    im = fix_dims(im)\n",
    "    if (center_face or crop_face) and not landmarks:\n",
    "        landmarks = fa.get_landmarks_from_image(im)\n",
    "    if (center_face or crop_face) and landmarks:\n",
    "        rects = create_bounding_box(landmarks, expansion_factor=expansion_factor)\n",
    "        x0,y0,w,h = sorted(rects, key=lambda x: x[2]*x[3])[-1]\n",
    "        if crop_face:\n",
    "            s = max(h, w)\n",
    "            x0 += (w-s)//2\n",
    "            x1 = x0 + s\n",
    "            y0 += (h-s)//2\n",
    "            y1 = y0 + s\n",
    "        else:\n",
    "            img_h,img_w = im.shape[:2]\n",
    "            img_s = min(img_h,img_w)\n",
    "            x0 = min(max(0, x0+(w-img_s)//2), img_w-img_s)\n",
    "            x1 = x0 + img_s\n",
    "            y0 = min(max(0, y0+(h-img_s)//2), img_h-img_s)\n",
    "            y1 = y0 + img_s            \n",
    "    else:\n",
    "        h,w = im.shape[:2]\n",
    "        s = min(h,w)\n",
    "        x0 = (w-s)//2\n",
    "        x1 = x0 + s\n",
    "        y0 = (h-s)//2\n",
    "        y1 = y0 + s\n",
    "    return int(x0),int(x1),int(y0),int(y1)\n",
    "\n",
    "def pad_crop_resize(im, x0=None, x1=None, y0=None, y1=None, new_h=256, new_w=256):\n",
    "    im = fix_dims(im)\n",
    "    h,w = im.shape[:2]\n",
    "    if x0 is None:\n",
    "      x0 = 0\n",
    "    if x1 is None:\n",
    "      x1 = w\n",
    "    if y0 is None:\n",
    "      y0 = 0\n",
    "    if y1 is None:\n",
    "      y1 = h\n",
    "    if x0<0 or x1>w or y0<0 or y1>h:\n",
    "        im = np.pad(im, pad_width=[(max(-y0,0),max(y1-h,0)),(max(-x0,0),max(x1-w,0)),(0,0)], mode='edge')\n",
    "    return resize(im[max(y0,0):y1-min(y0,0),max(x0,0):x1-min(x0,0)], (new_h, new_w))\n",
    "\n",
    "def get_crop_body(im, center_body=True, crop_body=True, expansion_factor=1, rects=None):\n",
    "    im = fix_dims(im)\n",
    "    if (center_body or crop_body) and rects is None:\n",
    "        rects, _ = hog.detectMultiScale(im, winStride=(4, 4),padding=(8,8), scale=expansion_factor)\n",
    "    if (center_body or crop_body) and rects is not None and len(rects):\n",
    "        x0,y0,w,h = sorted(rects, key=lambda x: x[2]*x[3])[-1]\n",
    "        if crop_body:\n",
    "            x0 += w//2-h//2\n",
    "            x1 = x0+h\n",
    "            y1 = y0+h\n",
    "        else:\n",
    "            img_h,img_w = im.shape[:2]\n",
    "            x0 += (w-img_h)//2\n",
    "            x1 = x0+img_h\n",
    "            y0 = 0\n",
    "            y1 = img_h\n",
    "    else:\n",
    "        h,w = im.shape[:2]\n",
    "        x0 = (w-h)//2\n",
    "        x1 = (w+h)//2\n",
    "        y0 = 0\n",
    "        y1 = h\n",
    "    return int(x0),int(x1),int(y0),int(y1)\n",
    "\n",
    "def crop_resize(im, size, crop=False):\n",
    "  if im.shape[:2] == size:\n",
    "    return im\n",
    "  if size[0]<im.shape[0] or size[1]<im.shape[1]:\n",
    "    interp = cv2.INTER_AREA\n",
    "  else:\n",
    "    interp = cv2.INTER_CUBIC\n",
    "  if not crop:\n",
    "    return np.clip(cv2.resize(im, size[::-1], interpolation=interp),0,1)\n",
    "  ratio = max(size[0]/im.shape[0], size[1]/im.shape[1])\n",
    "  im = np.clip(cv2.resize(im, (int(np.ceil(im.shape[1]*ratio)), int(np.ceil(im.shape[0]*ratio))), interpolation=interp),0,1)\n",
    "  return im[(im.shape[0]-size[0])//2:(im.shape[0]-size[0])//2+size[0], (im.shape[1]-size[1])//2:(im.shape[1]-size[1])//2+size[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "f3zKZ1bDDYSe"
   },
   "outputs": [],
   "source": [
    "#@title Define GANSpace functions\n",
    "#### 定义 GANSpace 函数\n",
    "\n",
    "from ipywidgets import fixed\n",
    "#ipywidgets: 交互式输入用到的包\n",
    "\n",
    "# Taken from https://github.com/alexanderkuk/log-progress\n",
    "# 显示进程\n",
    "def log_progress(sequence, every=1, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )\n",
    "\n",
    "# # name的潜在空间\n",
    "# def name_direction(sender):\n",
    "#   if not text.value:\n",
    "#     print('Please name the direction before saving')\n",
    "#     return\n",
    "    \n",
    "#   if num in named_directions.values():\n",
    "#     target_key = list(named_directions.keys())[list(named_directions.values()).index(num)]\n",
    "#     print(f'Direction already named: {target_key}')\n",
    "#     print(f'Overwriting... ')\n",
    "#     del(named_directions[target_key])\n",
    "#   named_directions[text.value] = [num, start_layer.value, end_layer.value]\n",
    "#   save_direction(random_dir, text.value)\n",
    "#   for item in named_directions:\n",
    "#     print(item, named_directions[item])\n",
    "\n",
    "    \n",
    "# def save_direction(direction, filename):\n",
    "#   filename += \".npy\"\n",
    "#   np.save(filename, direction, allow_pickle=True, fix_imports=True)\n",
    "#   print(f'Latent direction saved as {filename}')\n",
    "\n",
    "# def project_image(target_image, step, model, center=False, seed=303, use_clip=True, video=False): ######修改\n",
    "#     if model == 'portrait':\n",
    "#         pkl = '/content/drive/MyDrive/Colab/models/portrait-001000.pkl'\n",
    "#     elif model == 'character':\n",
    "#         pkl = '/content/drive/MyDrive/Colab/models/character-002600.pkl'\n",
    "#     elif model == 'model':\n",
    "#         pkl = '/content/drive/MyDrive/Colab/models/modelv4-001600.pkl'\n",
    "#     elif model == 'lookbook':\n",
    "#         pkl = '/content/drive/MyDrive/Colab/models/lookbook-001800.pkl'\n",
    "#     else:\n",
    "#         print('Model PKL file does not exists')\n",
    "\n",
    "#     !python /content/drive/MyDrive/Colab/fyp/stylegan2-ada-pytorch/pbaylies_projector.py --network={pkl} --outdir=/content/projector_output/ --target-image={target_image} --num-steps={step} --use-clip={use_clip} --use-center={center} --seed={seed} --save-video={video}\n",
    "\n",
    "#     projected_w = np.load('/content/projector_output/projected_w.npz')['w'].squeeze()\n",
    "#     return projected_w\n",
    "\n",
    "# def mix_w(w1, w2, content, style):\n",
    "#     for i in range(0,5):\n",
    "#         w2[i] = w1[i] * (1 - content) + w2[i] * content\n",
    "\n",
    "#     for i in range(5, 16):\n",
    "#         w2[i] = w1[i] * (1 - style) + w2[i] * style\n",
    "    \n",
    "#     return w2\n",
    "\n",
    "# 保存、展示、返回图片\n",
    "def display_sample_pytorch(seed=None, truncation=0.5, directions=None, distances=None, scale=1, start=0, end=14, w=None, disp=True, save=None):\n",
    "    # blockPrint()\n",
    "    model.truncation = truncation\n",
    "    \n",
    "    if w is None:\n",
    "        w = model.sample_latent(1, seed=seed).detach().cpu().numpy() # samplel:样本；即输入网络的图片，在人脸属性编辑是为1，在人脸融合时为2\n",
    "        w = [w]*model.get_max_latents() # one per layer\n",
    "    else:\n",
    "        w = [np.expand_dims(x, 0) for x in w]\n",
    "    \n",
    "    if directions != None and distances != None:\n",
    "        for l in range(start, end): # 从start层到end层\n",
    "          for i in range(len(directions)):\n",
    "            w[l] = w[l] + directions[i] * distances[i] * scale\n",
    "                        #  属性方向        变化距离\n",
    "            \n",
    "    \n",
    "    torch.cuda.empty_cache() # 删除一些不需要的变量代\n",
    "    #save image and display\n",
    "    out = model.sample_np(w)\n",
    "    \n",
    "    \n",
    "    final_im = Image.fromarray((out * 255).astype(np.uint8)).resize((500,500),Image.LANCZOS) # 实现array到image的转换\n",
    "    \n",
    "    \n",
    "    if save is not None:# 保存图片\n",
    "      if disp == False:\n",
    "        print(save)\n",
    "      final_im.save(f'out/{seed}_{save:05}.png')\n",
    "    if disp: # 显示图片\n",
    "      display(final_im)\n",
    "    \n",
    "    return final_im \n",
    "\n",
    "# # 生成视频\n",
    "# def generate_mov(seed, truncation, direction_vec, scale, layers, n_frames, out_name = 'out', noise_spec = None, loop=True):\n",
    "#   \"\"\"Generates a mov moving back and forth along the chosen direction vector\"\"\"\n",
    "#   # Example of reading a generated set of images, and storing as MP4.\n",
    "#   %mkdir out\n",
    "#   movieName = f'out/{out_name}.mp4'\n",
    "#   offset = -10\n",
    "#   step = 20 / n_frames\n",
    "#   imgs = []\n",
    "#   for i in log_progress(range(n_frames), name = \"Generating frames\"):\n",
    "#     print(f'\\r{i} / {n_frames}', end='')\n",
    "#     w = model.sample_latent(1, seed=seed).cpu().numpy()\n",
    "\n",
    "#     model.truncation = truncation\n",
    "#     w = [w]*model.get_max_latents() # one per layer\n",
    "#     for l in layers:\n",
    "#       if l <= model.get_max_latents():\n",
    "#           w[l] = w[l] + direction_vec * offset * scale\n",
    "\n",
    "#     #save image and display\n",
    "#     out = model.sample_np(w)\n",
    "#     final_im = Image.fromarray((out * 255).astype(np.uint8))\n",
    "#     imgs.append(out)\n",
    "#     #increase offset\n",
    "#     offset += step\n",
    "#   if loop:\n",
    "#     imgs += imgs[::-1]\n",
    "#   with imageio.get_writer(movieName, mode='I') as writer:\n",
    "#     for image in log_progress(list(imgs), name = \"Creating animation\"):\n",
    "#         writer.append_data(img_as_ubyte(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-S4Syb5DSUi",
    "outputId": "451abe62-67c1-4d4e-a1e3-9af64614b74c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title Load Model for GANSpace\n",
    "selected_model = 'character' #@param [\"portrait\", \"character\", \"model\", \"lookbook\"]\n",
    "\n",
    "# Load model\n",
    "from IPython.utils import io\n",
    "import torch\n",
    "import PIL # 该软件包提供了基本的图像处理功能，如：改变图像大小，旋转图像，图像格式转换，色场空间转换，图像增强，直方图处理，插值和滤波等等\n",
    "import numpy as np\n",
    "import ipywidgets as widgets # 用于jupyter笔记本和ipython内核的交互式html小部件\n",
    "from PIL import Image\n",
    "import imageio # 提供了一个易于阅读和 编写广泛的图像数据，包括动画图像、体积 数据和科学格式\n",
    "from models import get_instrumented_model\n",
    "from decomposition import get_or_compute\n",
    "from config import Config # 网络的配置\n",
    "from skimage import img_as_ubyte# 基于python脚本语言开发的数字图片处理包\n",
    "\n",
    "# Speed up computation\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Specify model to use\n",
    "config = Config(\n",
    "  model='StyleGAN2',\n",
    "  layer='style',\n",
    "  output_class=selected_model,\n",
    "  components=80,\n",
    "  use_w=True,\n",
    "  batch_size=5_000, # style layer quite small\n",
    ")\n",
    "\n",
    "inst = get_instrumented_model(config.model, config.output_class,\n",
    "                              config.layer, torch.device('cuda'), use_w=config.use_w)\n",
    "\n",
    "path_to_components = get_or_compute(config, inst) \n",
    "# 如果需要，返回缓存结果或计算\n",
    "# 传递现有的 Instrumented Model 实例以重用它\n",
    "\n",
    "model = inst.model # styleGAN2\n",
    "\n",
    "comps = np.load(path_to_components) # 加载主成分分析后的成分  npz文件\n",
    "# print(len(comps)) # 8\n",
    "# print(len(comps['act_comp'])) # 80\n",
    "# print(len(comps['act_comp'][0][0])) # 512\n",
    "# print(comps['act_comp'])\n",
    "# print(path_to_components) # gancreate-saai-main/cache/components/stylegan2-character_style_ipca_c80_n300000_w.npz\n",
    "\n",
    "\n",
    "lst = comps.files\n",
    "# print(lst)\n",
    "\n",
    "latent_dirs = []\n",
    "latent_stdevs = []\n",
    "\n",
    "load_activations = False\n",
    "\n",
    "for item in lst:\n",
    "    if load_activations:\n",
    "      if item == 'act_comp':\n",
    "        for i in range(comps[item].shape[0]):\n",
    "          latent_dirs.append(comps[item][i])\n",
    "      if item == 'act_stdev':\n",
    "        for i in range(comps[item].shape[0]):\n",
    "          latent_stdevs.append(comps[item][i])\n",
    "    else:\n",
    "      if item == 'lat_comp':\n",
    "        for i in range(comps[item].shape[0]):\n",
    "          latent_dirs.append(comps[item][i])\n",
    "      if item == 'lat_stdev':\n",
    "        for i in range(comps[item].shape[0]):\n",
    "          latent_stdevs.append(comps[item][i])\n",
    "# print(len(latent_dirs)) # 80: 取前80个主成分的方向\n",
    "# print(len(latent_dirs[0])) # 1\n",
    "# print(len(latent_dirs[0][0])) # 512：latent code的维度即主成分的方向\n",
    "\n",
    "# print(latent_stdevs) # [ ,80]\n",
    "\n",
    "def load_model(output_class):\n",
    "    global config\n",
    "    global inst\n",
    "    global model\n",
    "    config = Config(\n",
    "    model='StyleGAN2',\n",
    "    layer='style',\n",
    "    output_class=output_class,\n",
    "    components=80,\n",
    "    use_w=True,\n",
    "    batch_size=5_000, # style layer quite small\n",
    "  )\n",
    "\n",
    "    inst = get_instrumented_model(config.model, config.output_class,\n",
    "                                  config.layer, torch.device('cuda'), use_w=config.use_w)\n",
    "\n",
    "    path_to_components = get_or_compute(config, inst)\n",
    "\n",
    "    model = inst.model\n",
    "\n",
    "    comps = np.load(path_to_components)\n",
    "    lst = comps.files\n",
    "    latent_dirs = []\n",
    "    latent_stdevs = []\n",
    "\n",
    "    load_activations = False\n",
    "\n",
    "    for item in lst:\n",
    "        if load_activations:\n",
    "          if item == 'act_comp':\n",
    "            for i in range(comps[item].shape[0]):\n",
    "              latent_dirs.append(comps[item][i])\n",
    "          if item == 'act_stdev':\n",
    "            for i in range(comps[item].shape[0]):\n",
    "              latent_stdevs.append(comps[item][i])\n",
    "        else:\n",
    "          if item == 'lat_comp':\n",
    "            for i in range(comps[item].shape[0]):\n",
    "              latent_dirs.append(comps[item][i])\n",
    "          if item == 'lat_stdev':\n",
    "        \n",
    "            for i in range(comps[item].shape[0]):\n",
    "              latent_stdevs.append(comps[item][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import prettify_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP6w_g9iez9m"
   },
   "source": [
    "### GANSpace UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "oUti6B_uDite",
    "outputId": "8167ad77-0f4d-48a0-ef8a-df8e203961f5"
   },
   "outputs": [],
   "source": [
    "# #@title Portrait GANSpace UI\n",
    "# import gradio as gr\n",
    "# import numpy as np\n",
    "\n",
    "# def generate_image(seed, truncation,\n",
    "#                   female, realism, greyhair, shorthair, shortchin, ponytail, blackhair,\n",
    "#                   start_layer, end_layer):\n",
    "\n",
    "#     scale = 1\n",
    "#     seed, start_layer, end_layer = int(seed), int(start_layer), int(end_layer)\n",
    "#     w = None\n",
    "#     params = {'female': female,\n",
    "#           'realism': realism,\n",
    "#           'greyhair': greyhair,\n",
    "#           'shorthair': shorthair,\n",
    "#           'shortchin': shortchin,\n",
    "#           'ponytail': ponytail,\n",
    "#           'blackhair': blackhair}\n",
    "\n",
    "#     param_indexes = {'female': 1,\n",
    "#               'realism': 4,\n",
    "#               'greyhair': 5,\n",
    "#               'shorthair': 6,\n",
    "#               'shortchin': 8,\n",
    "#               'ponytail': 9,\n",
    "#               'blackhair': 10}\n",
    "\n",
    "#     directions = []\n",
    "#     distances = []\n",
    "#     for k, v in params.items():\n",
    "#         directions.append(latent_dirs[param_indexes[k]])\n",
    "#         distances.append(v)\n",
    "\n",
    "#     model.truncation = truncation\n",
    "#     if w is None:\n",
    "#         w = model.sample_latent(1, seed=seed).detach().cpu().numpy()\n",
    "#         w = [w]*model.get_max_latents() # one per layer\n",
    "#     else:\n",
    "#         w = [np.expand_dims(x, 0) for x in w]\n",
    "    \n",
    "\n",
    "#     if directions != None and distances != None:\n",
    "#         for l in range(start_layer, end_layer):\n",
    "#           for i in range(len(directions)):\n",
    "#             w[l] = w[l] + directions[i] * distances[i] * 1\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "#     #save image and display\n",
    "#     out = model.sample_np(w)\n",
    "#     final_im = Image.fromarray((out * 255).astype(np.uint8))\n",
    "#     final_im.save('/tmp/edit_output.jpg') # save the content to temp\n",
    "# #     print(\"saved\")\n",
    "    \n",
    "#     return final_im\n",
    "\n",
    "#     #return display_sample_pytorch(int(seed), truncation, directions, distances, scale, int(start_layer), int(end_layer), disp=False)\n",
    "\n",
    "# truncation = gr.inputs.Slider(minimum=0, maximum=1, default=0.5, label=\"Truncation\")\n",
    "# start_layer = gr.inputs.Number(default=0, label=\"Start Layer\")\n",
    "# end_layer = gr.inputs.Number(default=14, label=\"End Layer\")\n",
    "# seed = gr.inputs.Number(default=0, label=\"Seed\")\n",
    "\n",
    "# slider_max_val = 20\n",
    "# slider_min_val = -20\n",
    "# slider_step = 1\n",
    "\n",
    "# female = gr.inputs.Slider(label=\"Female\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "# realism = gr.inputs.Slider(label=\"Realism\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "# greyhair = gr.inputs.Slider(label=\"Grey Hair\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "# shorthair = gr.inputs.Slider(label=\"Short Hair\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "# shortchin = gr.inputs.Slider(label=\"Short Chin\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "# ponytail = gr.inputs.Slider(label=\"Ponytail\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "# blackhair = gr.inputs.Slider(label=\"Black Hair\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "\n",
    "\n",
    "# scale = 1\n",
    "\n",
    "# inputs = [seed, truncation, female, realism, greyhair, shorthair, shortchin, ponytail, blackhair, start_layer, end_layer]\n",
    "\n",
    "# gr.Interface(generate_image, inputs, \"image\", live=True, title=\"GAN Editing\").launch(debug=True,share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pR_VVuot7pn0",
    "outputId": "b1a6f8e6-838c-48f5-e391-b9cb7198a2d4"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "seed = 'asds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "0DzGvutIfJdG",
    "outputId": "f09e56ea-14d4-42f9-ab81-f79608f1a450"
   },
   "outputs": [],
   "source": [
    "#@title Character Gradio UI\n",
    "import gradio as gr # 一个开源的构建自动化工具\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "def generate_image(seed, truncation,\n",
    "                  monster, female, skimpy, light, bodysuit, bulky, human_head,\n",
    "                  start_layer, end_layer):\n",
    "\n",
    "    scale = 1\n",
    "    seed = int(hashlib.sha256(seed.encode('utf-8')).hexdigest(), 16) % 10**8\n",
    "\n",
    "    params = {'monster': monster,\n",
    "          'female': female,\n",
    "          'skimpy': skimpy,\n",
    "          'light': light,\n",
    "          'bodysuit': bodysuit,\n",
    "          'bulky': bulky,\n",
    "          'human_head': human_head}\n",
    "\n",
    "    param_indexes = {'monster': 0,\n",
    "              'female': 1,\n",
    "              'skimpy': 2,\n",
    "              'light': 4,\n",
    "              'bodysuit': 5,\n",
    "              'bulky': 6,\n",
    "              'human_head': 8}\n",
    "\n",
    "    directions = []\n",
    "    distances = []\n",
    "    for k, v in params.items():\n",
    "        directions.append(latent_dirs[param_indexes[k]])  # 对应主成分的方向\n",
    "        distances.append(v) # 对应主成分改变的距离\n",
    "#     print(distances)\n",
    "\n",
    "    style = {'description_width': 'initial'}\n",
    "    return display_sample_pytorch(int(seed), truncation, directions, distances, scale, int(start_layer), int(end_layer), disp=False)\n",
    "\n",
    "truncation = gr.inputs.Slider(minimum=0, maximum=1, default=0.5, label=\"Truncation\")\n",
    "start_layer = gr.inputs.Number(default=0, label=\"Start Layer\")\n",
    "end_layer = gr.inputs.Number(default=14, label=\"End Layer\")\n",
    "seed = gr.inputs.Textbox(default='0')\n",
    "\n",
    "slider_max_val = 20\n",
    "slider_min_val = -20\n",
    "slider_step = 1\n",
    "\n",
    "monster = gr.inputs.Slider(label=\"Monster\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "female = gr.inputs.Slider(label=\"Female\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "skimpy = gr.inputs.Slider(label=\"Skimpy\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "light = gr.inputs.Slider(label=\"Light\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "bodysuit = gr.inputs.Slider(label=\"Bodysuit\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "bulky = gr.inputs.Slider(label=\"Bulky\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "human_head = gr.inputs.Slider(label=\"Human Head\", minimum=slider_min_val, maximum=slider_max_val, default=0)\n",
    "\n",
    "\n",
    "scale = 1\n",
    "\n",
    "inputs = [seed, truncation, monster, female, skimpy, light, bodysuit, bulky, human_head, start_layer, end_layer]\n",
    "\n",
    "gr.Interface(generate_image, inputs, \"image\", live=True, title=\"CharacterGAN\").launch(debug=True,share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG0KVnIGfH5K"
   },
   "source": [
    "### FOMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import ParmSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rMmcGdYMDnyo"
   },
   "outputs": [],
   "source": [
    "#@title Load Checkpoint for FOMM\n",
    "from demo import load_checkpoints\n",
    "import moviepy.editor as mpe\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from demo import make_animation\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "#!gdown --id 1jmcn19-c3p8mf39aYNXUhdMqzqDYZhQ_ -O vox-cpk.pth.tar\n",
    "\n",
    "generator, kp_detector = load_checkpoints(config_path='/content/drive/MyDrive/Colab/fyp/first-order-model/config/vox-256.yaml', \n",
    "                            checkpoint_path='/content/drive/MyDrive/Colab/fyp/first-order-model/vox-cpk.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6O5jtu6_Ds8R",
    "outputId": "a4cbd547-ef3f-49ab-e366-dd9cfe9bb193"
   },
   "outputs": [],
   "source": [
    "#@title FOMM UI\n",
    "import gradio as gr\n",
    "import moviepy.editor as mpe\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from demo import make_animation\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "def predict_video(image_path, video_path, relative, show_inputs,\n",
    "                  start_time=0, end_time=-1,\n",
    "                  center_video_to_head=True, crop_video_to_head=True, video_crop_expansion_factor=1.5,\n",
    "                  center_image_to_head=True, crop_image_to_head=True, image_crop_expansion_factor=2.5,\n",
    "                  speech=\"\", speaker='p225'):\n",
    "  \n",
    "    video_crop_expansion_factor = max(video_crop_expansion_factor, 1)\n",
    "    image_crop_expansion_factor = max(image_crop_expansion_factor, 1)\n",
    "\n",
    "    if end_time > start_time:\n",
    "        # cut video\n",
    "        print('Cutting Video...')\n",
    "        ffmpeg_extract_subclip(video_path, start_time, end_time, targetname='cut.mp4')\n",
    "        video_path = 'cut.mp4'\n",
    "\n",
    "    source_image = imageio.imread(image_path.name)\n",
    "    reader = imageio.get_reader(video_path)\n",
    "\n",
    "    source_image = pad_crop_resize(source_image, *get_crop(source_image, center_face=center_image_to_head, crop_face=crop_image_to_head, expansion_factor=image_crop_expansion_factor))\n",
    "    fps = reader.get_meta_data()['fps']\n",
    "    print('FPS', fps)\n",
    "    #fps = 8\n",
    "\n",
    "    driving_video = []\n",
    "    landmarks = None\n",
    "    try:\n",
    "        for i,im in enumerate(reader):\n",
    "            if not crop_video_to_head:\n",
    "                break\n",
    "            landmarks = fa.get_landmarks_from_image(im)\n",
    "            if landmarks:\n",
    "                break\n",
    "        x0,x1,y0,y1 = get_crop(im, center_face=center_video_to_head, crop_face=crop_video_to_head, expansion_factor=video_crop_expansion_factor, landmarks=landmarks)\n",
    "        reader.set_image_index(0)\n",
    "        for im in reader:\n",
    "            driving_video.append(pad_crop_resize(im,x0,x1,y0,y1))\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    # Generate animation\n",
    "    predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=relative)\n",
    "\n",
    "    fig = plt.figure(figsize=(10 * show_inputs + 8 * (predictions is not None), 8))\n",
    "\n",
    "    ims = []\n",
    "    for i in range(len(driving_video)):\n",
    "        cols = []\n",
    "        if show_inputs and speech == \"\":\n",
    "            cols.append(source_image)\n",
    "            cols.append(driving_video[i])\n",
    "        if predictions is not None:\n",
    "            cols.append(predictions[i])\n",
    "        im = plt.imshow(np.concatenate(cols, axis=1), animated=True)\n",
    "        plt.axis('off')\n",
    "        ims.append([im])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
    "    ani.save('output.mp4', fps=fps)\n",
    "    plt.close()\n",
    "\n",
    "    if speech != \"\":\n",
    "        !tts --text \"{speech}\" --out_path speech.wav --model_name \"tts_models/en/vctk/sc-glow-tts\"  --speaker_idx {speaker}\n",
    "        !cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"../output.mp4\" --audio '../speech.wav'\n",
    "        return 'Wav2Lip/results/result_voice.mp4'\n",
    "    else:\n",
    "        video_clip = mpe.VideoFileClip('output.mp4')\n",
    "        audio_clip = mpe.AudioFileClip(video_path)\n",
    "        final_clip = video_clip.set_audio(audio_clip)\n",
    "        final_clip.write_videofile(\"result.mp4\", fps=fps)\n",
    "        return 'result.mp4'\n",
    "\n",
    "image_input = gr.inputs.Image(type=\"file\")\n",
    "video_input = gr.inputs.Video(type=\"mp4\")\n",
    "relative = gr.inputs.Checkbox(default=True, label=\"Relative\")\n",
    "show_inputs = gr.inputs.Checkbox(default=True, label=\"Show Inputs\")\n",
    "start_time = gr.inputs.Number(default=0, label=\"Start Time\")\n",
    "end_time = gr.inputs.Number(default=-1, label=\"End Time\")\n",
    "center_video_to_head = gr.inputs.Checkbox(default=True, label=\"Center Video to Head\")\n",
    "crop_video_to_head = gr.inputs.Checkbox(default=True, label=\"Crop Video to Head\")\n",
    "video_crop_expansion_factor = gr.inputs.Number(default=2, label=\"Video Crop Expansion Factor\")\n",
    "center_image_to_head = gr.inputs.Checkbox(default=False, label=\"Center Image to Head\")\n",
    "crop_image_to_head = gr.inputs.Checkbox(default=False, label=\"Crop Image to Head\")\n",
    "image_crop_expansion_factor = gr.inputs.Number(default=2, label=\"Image Crop Expansion Factor\")\n",
    "speech = gr.inputs.Textbox(label=\"Text to Speech\", default=\"\")\n",
    "speaker = gr.inputs.Dropdown(default='p225', choices=['p225', 'p226', 'p227', 'p228', 'p229', 'p230', 'p231', 'p232', 'p233', 'p234', 'p261'])\n",
    "\n",
    "inputs = [image_input, video_input, relative, show_inputs, start_time, end_time,\n",
    "          center_video_to_head, crop_video_to_head, video_crop_expansion_factor,\n",
    "          center_image_to_head, crop_image_to_head, image_crop_expansion_factor,\n",
    "          speech, speaker]\n",
    "    \n",
    "gr.Interface(predict_video, inputs, \"video\", live=False, title=\"Facial Animation\").launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzR0qLFXfMZb"
   },
   "source": [
    "### Impersonator++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yOfh0R7YGe5K",
    "outputId": "dc1f1667-bb26-4e6d-bd77-465f7cec950e"
   },
   "outputs": [],
   "source": [
    "#@title Impersonator++ Gradio UI\n",
    "import gradio as gr\n",
    "import os.path as osp\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import subprocess\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "def imitate(im, video, bg_image, background, \n",
    "                   center_image_to_body, crop_image_to_body, image_crop_expansion_factor, keep_aspect_background, show_inputs, pose_fc, cam_fc):\n",
    "    source_image = imageio.imread(im.name)\n",
    "    source_image = pad_crop_resize(source_image, *get_crop_body(source_image, center_body=center_image_to_body, crop_body=crop_image_to_body, expansion_factor=image_crop_expansion_factor), new_h=800, new_w=800)\n",
    "    imageio.imwrite('/content/crop.png', (source_image*255).astype(np.uint8))\n",
    "\n",
    "    if bg_image != 'None':\n",
    "        bg_image = imageio.imread(bg_image.name)\n",
    "        bg_image = crop_resize(bg_image/255, source_image.shape[:2], crop=keep_aspect_background)\n",
    "        imageio.imwrite('/content/bg_crop.png', (bg_image*255).astype(np.uint8))\n",
    "    \n",
    "    with imageio.get_reader(video, format='mp4') as reader:\n",
    "        fps = reader.get_meta_data()['fps']\n",
    "    \n",
    "    gpu_ids = \"0\"\n",
    "    num_source = 1\n",
    "    assets_dir = \"/content/drive/MyDrive/Colab/fyp/iPERCore/assets\"\n",
    "    output_dir = \"/content/drive/MyDrive/Colab/fyp/iPERCore/results\"\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "\n",
    "    # symlink from the actual assets directory to this current directory\n",
    "    work_asserts_dir = os.path.join(\"./assets\")\n",
    "    if not os.path.exists(work_asserts_dir):\n",
    "        os.symlink(osp.abspath(assets_dir), osp.abspath(work_asserts_dir),\n",
    "                  target_is_directory=(platform.system() == \"Windows\"))\n",
    "\n",
    "    cfg_path = osp.join(work_asserts_dir, \"configs\", \"deploy.toml\")\n",
    "\n",
    "    model_id = \"mymodel\"\n",
    "    \n",
    "    src_path = \"\\\"path?=/content/crop.png,name?=mymodel\"\n",
    "    if background=='replace' and os.path.exists('/content/bg_crop.png'):\n",
    "      src_path += ',bg_path?=/content/bg_crop.png'\n",
    "    src_path += '\"'\n",
    "\n",
    "    ref_path = \"\\\"path?=%s,\"  \\\n",
    "             \"name?=myoutput,\" \\\n",
    "             \"pose_fc?=%d,\"\\\n",
    "             \"cam_fc?=%d,\"\\\n",
    "             \"fps?=%f\\\"\"%(video,pose_fc,cam_fc,fps)\n",
    "    options = ''\n",
    "    if background=='inpaint':\n",
    "        options += ' --use_inpaintor'\n",
    "    \n",
    "    !python -m iPERCore.services.run_imitator --gpu_ids $gpu_ids --num_source $num_source --image_size $image_size --output_dir $output_dir --model_id $model_id --cfg_path $cfg_path --src_path $src_path --ref_path $ref_path $options\n",
    "\n",
    "    if show_inputs:\n",
    "        return \"/content/drive/MyDrive/Colab/fyp/iPERCore/results/primitives/mymodel/synthesis/imitations/mymodel-myoutput.mp4\"\n",
    "    else:\n",
    "        result_dir = '/content/drive/MyDrive/Colab/fyp/iPERCore/results/primitives/mymodel/synthesis/imitations/mymodel-myoutput'\n",
    "        frames = [os.path.abspath(os.path.join(result_dir, p)) for p in os.listdir(result_dir) if p.endswith(('jpg', 'png'))]\n",
    "        frames.sort()\n",
    "\n",
    "        #fps = last_frame/10\n",
    "        clip = ImageSequenceClip(frames, fps = fps)\n",
    "\n",
    "        #import re\n",
    "        #output_file = re.compile('\\.png$').sub('.mp4', args.output)\n",
    "        clip.write_videofile('impersonator_output.mp4')\n",
    "        return 'impersonator_output.mp4'\n",
    "\n",
    "\n",
    "im = gr.inputs.Image(type=\"file\", label=\"Source Image\")\n",
    "bg_image = gr.inputs.Image(type=\"file\", label=\"Background Image\")\n",
    "background = gr.inputs.Radio(choices=['None', 'replace', 'inpaint'], label=\"Background\")\n",
    "center_image_to_body = gr.inputs.Checkbox(default=True, label=\"Center Image to Body\")\n",
    "crop_image_to_body = gr.inputs.Checkbox(default=False, label=\"Crop Image to Body\")\n",
    "image_crop_expansion_factor = gr.inputs.Number(default=1.05, label=\"Image Crop Expansion Factor\")\n",
    "keep_aspect_background = gr.inputs.Checkbox(default=True, label=\"Keep Background Aspect Ratio\")\n",
    "show_inputs = gr.inputs.Checkbox(default=False, label=\"Show Inputs\")\n",
    "pose_fc = gr.inputs.Number(default=300, label=\"Pose Smooth Factor\")\n",
    "cam_fc = gr.inputs.Number(default=100, label=\"Camera Smooth Factor\")\n",
    "\n",
    "inputs = [im, 'video', bg_image, background, \n",
    "          center_image_to_body, crop_image_to_body, image_crop_expansion_factor, keep_aspect_background, show_inputs, pose_fc, cam_fc]\n",
    "\n",
    "\n",
    "\n",
    "iface = gr.Interface(imitate, inputs, \"video\", live=False, title=\"Full Body Animation\")\n",
    "iface.launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mUwHxVFTDEg"
   },
   "source": [
    "\n",
    "## Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "krm-eciaTx5M",
    "outputId": "805bc540-115d-4eef-8517-a69272bc2ded"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "# display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 200})'''))\n",
    "\n",
    "!pip install fastapi nest-asyncio pyngrok uvicorn aiofiles python-multipart firebase-admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXYdqPx18Iv0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "firebase_admin.delete_app(firebase_admin.get_app())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_n4mDrn_UBO5"
   },
   "outputs": [],
   "source": [
    "#@title API Functions\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import FileResponse, StreamingResponse\n",
    "from fastapi import FastAPI, File, UploadFile, Form\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Dict, List\n",
    "from io import StringIO, BytesIO\n",
    "from pydantic import BaseModel\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore, storage\n",
    "\n",
    "# Use the application default credentials\n",
    "if not firebase_admin._apps:\n",
    "    cred = credentials.Certificate(\"/content/drive/MyDrive/Colab/fyp/gancreate-firebase-adminsdk-yayqe-07b54a912d.json\")\n",
    "    firebase_admin.initialize_app(cred, {\n",
    "        'storageBucket': 'gancreate.appspot.com'\n",
    "    })\n",
    "\n",
    "db = firestore.client()\n",
    "bucket = storage.bucket()\n",
    "\n",
    "\n",
    "# Log the user in\n",
    "user = \"YyzhczpqEzMw6uWHuikt0Icg2Sp1\"\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "@app.get('/')\n",
    "async def root():\n",
    "    return {'hello': 'world'}\n",
    "\n",
    "def sample(seed: int = None, truncation: float = 0.5, w=None):\n",
    "    model.truncation = truncation\n",
    "    if w is None:\n",
    "        w = model.sample_latent(1, seed=seed).detach().cpu().numpy()\n",
    "        w = [w]*model.get_max_latents() # one per layer\n",
    "  \n",
    "    #save image and display\n",
    "    out = model.sample_np(w)\n",
    "    return Image.fromarray((out * 255).astype(np.uint8))\n",
    "\n",
    "@app.get('/generate')\n",
    "async def generate(model_type: str = 'portrait', seed: int = None, truncation: float = 0.5, w=None, save=False):\n",
    "    if config.output_class != model_type:\n",
    "        load_model(model_type)\n",
    "    model.truncation = truncation\n",
    "    if w is None:\n",
    "        w = model.sample_latent(1, seed=seed).detach().cpu().numpy()\n",
    "        w = [w]*model.get_max_latents() # one per layer\n",
    "  \n",
    "    #save image and display\n",
    "    out = model.sample_np(w)\n",
    "    final_im = Image.fromarray((out * 255).astype(np.uint8))\n",
    "    \n",
    "    final_im.save('/tmp/output.jpg') # save the content to temp\n",
    "    \n",
    "    return FileResponse('/tmp/output.jpg', media_type=\"image/jpeg\")\n",
    "\n",
    "class EditConfig(BaseModel):\n",
    "    model_type: Optional[str] = \"portrait\" \n",
    "    seed: Optional[int] = None\n",
    "    truncation: Optional[float] = 0.5\n",
    "    start_layer: Optional[int] = 0\n",
    "    end_layer: Optional[int] = 14\n",
    "    attributes: Dict[str, float]\n",
    "    latent_id: Optional[str] = \"\"\n",
    "    save: Optional[bool] = False\n",
    "\n",
    "@app.post('/edit')\n",
    "async def edit(edit_config: EditConfig):\n",
    "    model_type = edit_config.model_type\n",
    "    if config.output_class != model_type:\n",
    "        load_model(model_type)\n",
    "\n",
    "    seed = edit_config.seed        \n",
    "    latent_id = edit_config.latent_id\n",
    "    model.truncation = edit_config.truncation\n",
    "    if latent_id is None or latent_id is \"\":\n",
    "        w = model.sample_latent(1, seed=seed).detach().cpu().numpy()\n",
    "        w = [w]*model.get_max_latents() # one per layer\n",
    "    else:\n",
    "        w = np.load(f\"latents/{model_type}/{latent_id}.npy\")\n",
    "        w = [np.expand_dims(x, 0) for x in w]\n",
    "    \n",
    "    param_indexes = {\n",
    "        \"portrait\": {\n",
    "            'Gender': 1,\n",
    "            'Realism': 4,\n",
    "            'Gray Hair': 5,\n",
    "            'Hair Length': 6,\n",
    "            'Chin': 8,\n",
    "            'Ponytail': 9,\n",
    "            'Black Hair': 10\n",
    "        },\n",
    "        \"model\": {\n",
    "            'Gender': 4,\n",
    "            'Dress': 0,\n",
    "            'Sleveeless': 1,\n",
    "            'Short Skirt': 3,\n",
    "            'Jacket': 5,\n",
    "            'Darkness': 7,\n",
    "            'Slimness': 9\n",
    "        },\n",
    "        \"character\": {\n",
    "            'Monster': 0,\n",
    "            'Gender': 1,\n",
    "            'Skimpiness': 2,\n",
    "            'Light': 4,\n",
    "            'Bodysuit': 5,\n",
    "            'Bulkiness': 6,\n",
    "            'Human Head': 8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    directions = []\n",
    "    distances = []\n",
    "    for k, v in edit_config.attributes.items():\n",
    "        directions.append(latent_dirs[param_indexes[model_type][k]])\n",
    "        distances.append(v)\n",
    "\n",
    "    if directions != None and distances != None:\n",
    "        for l in range(edit_config.start_layer, edit_config.end_layer):\n",
    "          for i in range(len(directions)):\n",
    "            w[l] = w[l] + directions[i] * distances[i] * 1\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    #save image and display\n",
    "    out = model.sample_np(w)\n",
    "    final_im = Image.fromarray((out * 255).astype(np.uint8))\n",
    "    final_im.save('/tmp/edit_output.jpg') # save the content to temp\n",
    "\n",
    "    if edit_config.save:\n",
    "        doc_ref = db.collection(f'{model_type}s').add({})\n",
    "        doc_id = doc_ref[1].id\n",
    "        \n",
    "        blob = bucket.blob(f'{model_type}s/{doc_id}.jpg')\n",
    "        blob.upload_from_filename('/tmp/edit_output.jpg')\n",
    "        blob.make_public()\n",
    "\n",
    "        data = {\n",
    "            \"url\": blob.public_url,\n",
    "            \"user\": user,\n",
    "            \"created_at\": datetime.datetime.now()\n",
    "        }\n",
    "        db.collection(f'{model_type}s').document(doc_id).set(data)\n",
    "        np.save(f'latents/{model_type}/{doc_id}.npy', w)\n",
    "\n",
    "    return FileResponse('/tmp/edit_output.jpg', media_type=\"image/jpeg\")\n",
    "\n",
    "class AnimateFaceConfig(BaseModel):\n",
    "    center_video_to_head: Optional[bool] = True\n",
    "    crop_video_to_head: Optional[bool] = True\n",
    "    center_image_to_head: Optional[bool] = True\n",
    "    center_video_to_head: Optional[bool] = True\n",
    "    video_crop_expansion_factor: Optional[float] = 1.5\n",
    "    image_crop_expansion_factor: Optional[float] = 2.5\n",
    "    speech: Optional[str] = \"\"\n",
    "    speaker: Optional[str] = \"p225\"\n",
    "    show_inputs: Optional[bool] = False\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/animate/face\")\n",
    "async def animate_face(\n",
    "        files: List[UploadFile] = File(...),\n",
    "        center_video_to_head: bool = Form(True),\n",
    "        crop_video_to_head: bool = Form(True),\n",
    "        video_crop_expansion_factor: float = Form(1.5),\n",
    "        center_image_to_head: bool = Form(True),\n",
    "        crop_image_to_head: bool = Form(True),\n",
    "        image_crop_expansion_factor: float = Form(2.5),\n",
    "        relative: bool = Form(True),\n",
    "        speech: str = Form(\"\"),\n",
    "        speaker: str = Form(\"p225\"),\n",
    "        show_inputs: bool = Form(False),\n",
    "    ):\n",
    "  \n",
    "    video_crop_expansion_factor = max(video_crop_expansion_factor, 1)\n",
    "    image_crop_expansion_factor = max(image_crop_expansion_factor, 1)\n",
    "\n",
    "    source_image = files[0]\n",
    "    ref_video = files[1]\n",
    "\n",
    "    with open(source_image.filename, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(source_image.file, buffer)\n",
    "\n",
    "    video_path = \"ref_video.mp4\"\n",
    "\n",
    "    with open(video_path, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(ref_video.file, buffer)\n",
    "\n",
    "    source_image = imageio.imread(source_image.filename)\n",
    "    reader = imageio.get_reader(video_path)\n",
    "\n",
    "    source_image = pad_crop_resize(source_image, *get_crop(source_image, center_face=center_image_to_head, crop_face=crop_image_to_head, expansion_factor=image_crop_expansion_factor))\n",
    "    fps = reader.get_meta_data()['fps']\n",
    "\n",
    "    driving_video = []\n",
    "    landmarks = None\n",
    "    try:\n",
    "        for i,im in enumerate(reader):\n",
    "            if not crop_video_to_head:\n",
    "                break\n",
    "            landmarks = fa.get_landmarks_from_image(im)\n",
    "            if landmarks:\n",
    "                break\n",
    "        x0,x1,y0,y1 = get_crop(im, center_face=center_video_to_head, crop_face=crop_video_to_head, expansion_factor=video_crop_expansion_factor, landmarks=landmarks)\n",
    "        reader.set_image_index(0)\n",
    "        for im in reader:\n",
    "            driving_video.append(pad_crop_resize(im,x0,x1,y0,y1))\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    # Generate animation\n",
    "    predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=relative)\n",
    "\n",
    "    fig = plt.figure(figsize=(12 * show_inputs + 6 * (predictions is not None), 8))\n",
    "\n",
    "    ims = []\n",
    "    for i in range(len(driving_video)):\n",
    "        cols = []\n",
    "        if show_inputs and speech == \"\":\n",
    "            cols.append(source_image)\n",
    "            cols.append(driving_video[i])\n",
    "        if predictions is not None:\n",
    "            cols.append(predictions[i])\n",
    "        im = plt.imshow(np.concatenate(cols, axis=1), animated=True)\n",
    "        plt.axis('off')\n",
    "        ims.append([im])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
    "    ani.save('output.mp4', fps=fps)\n",
    "    plt.close()\n",
    "\n",
    "    if speech != \"\":\n",
    "        !tts --text \"{speech}\" --out_path speech.wav --model_name \"tts_models/en/vctk/sc-glow-tts\"  --speaker_idx {speaker}\n",
    "        !cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"../output.mp4\" --audio '../speech.wav'\n",
    "        return FileResponse('Wav2Lip/results/result_voice.mp4', media_type=\"video/mp4\")\n",
    "    else:\n",
    "        video_clip = mpe.VideoFileClip('output.mp4')\n",
    "        audio_clip = mpe.AudioFileClip(video_path)\n",
    "        final_clip = video_clip.set_audio(audio_clip)\n",
    "        final_clip.write_videofile(\"result.mp4\", fps=fps)\n",
    "        return FileResponse('result.mp4', media_type=\"video/mp4\")\n",
    "\n",
    "@app.post(\"/animate/body\")\n",
    "async def animate_body(\n",
    "        files: List[UploadFile] = File(...),\n",
    "        background: str = Form(\"None\"),\n",
    "        center_image_to_body: bool = Form(True),\n",
    "        crop_image_to_body: bool = Form(False),\n",
    "        image_crop_expansion_factor: float = Form(1.05) ,\n",
    "        keep_aspect_background: bool = Form(True),\n",
    "        pose_fc: int = Form(300),\n",
    "        cam_fc: int = Form(100),\n",
    "    ):\n",
    "  \n",
    "    source_image = files[0]\n",
    "    ref_video = files[1]\n",
    "    bg_image = files[2]\n",
    "\n",
    "    with open(source_image.filename, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(source_image.file, buffer)\n",
    "\n",
    "    video_path = \"ref_video.mp4\"\n",
    "\n",
    "    with open(video_path, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(ref_video.file, buffer)\n",
    "    \n",
    "    source_image = imageio.imread(source_image.filename)\n",
    "    source_image = pad_crop_resize(source_image, *get_crop_body(source_image, center_body=center_image_to_body, crop_body=crop_image_to_body, expansion_factor=image_crop_expansion_factor))\n",
    "    imageio.imwrite('/content/crop.png', (source_image*255).astype(np.uint8))\n",
    "\n",
    "    if bg_image != 'None':\n",
    "        with open(bg_image.filename, \"wb\") as buffer:\n",
    "            shutil.copyfileobj(bg_image.file, buffer)\n",
    "        bg_image = imageio.imread(bg_image.filename)\n",
    "        bg_image = crop_resize(bg_image/255, source_image.shape[:2], crop=keep_aspect_background)\n",
    "        imageio.imwrite('/content/bg_crop.png', (bg_image*255).astype(np.uint8))\n",
    "    \n",
    "    with imageio.get_reader(video_path, format='mp4') as reader:\n",
    "        fps = reader.get_meta_data()['fps']\n",
    "    \n",
    "    gpu_ids = \"0\"\n",
    "    num_source = 1\n",
    "    assets_dir = \"/content/drive/MyDrive/Colab/fyp/iPERCore/assets\"\n",
    "    output_dir = \"/content/drive/MyDrive/Colab/fyp/iPERCore/results\"\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "\n",
    "    # symlink from the actual assets directory to this current directory\n",
    "    work_asserts_dir = os.path.join(\"./assets\")\n",
    "    if not os.path.exists(work_asserts_dir):\n",
    "        os.symlink(osp.abspath(assets_dir), osp.abspath(work_asserts_dir),\n",
    "                  target_is_directory=(platform.system() == \"Windows\"))\n",
    "\n",
    "    cfg_path = osp.join(work_asserts_dir, \"configs\", \"deploy.toml\")\n",
    "\n",
    "    model_id = \"mymodel\"\n",
    "    \n",
    "    src_path = \"\\\"path?=/content/crop.png,name?=mymodel\"\n",
    "    if background=='replace' and os.path.exists('/content/bg_crop.png'):\n",
    "      src_path += ',bg_path?=/content/bg_crop.png'\n",
    "    src_path += '\"'\n",
    "\n",
    "    ref_path = \"\\\"path?=%s,\"  \\\n",
    "             \"name?=myoutput,\" \\\n",
    "             \"pose_fc?=%d,\"\\\n",
    "             \"cam_fc?=%d,\"\\\n",
    "             \"fps?=%f\\\"\"%(video_path,pose_fc,cam_fc,fps)\n",
    "    options = ''\n",
    "    if background=='inpaint':\n",
    "        options += ' --use_inpaintor'\n",
    "    \n",
    "    !python -m iPERCore.services.run_imitator --gpu_ids $gpu_ids --num_source $num_source --image_size $image_size --output_dir $output_dir --model_id $model_id --cfg_path $cfg_path --src_path $src_path --ref_path $ref_path $options\n",
    "\n",
    "    return FileResponse('./iPERCore/results/primitives/mymodel/synthesis/imitations/mymodel-myoutput.mp4', media_type=\"video/mp4\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRxuMq-8UgXa",
    "outputId": "ab137b69-28f1-4826-94e1-f6c03cc6090c"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url+'/docs')\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMBJ8YaOSgd-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UGb-pRdlKcjD",
    "wI3qY3ocKiU7",
    "ipK1spN2b2XY",
    "se4UBp6bjCNV"
   ],
   "name": "Copy of FYP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ab6ef4cd6d868f72b0fea9aaf51250926d855dd9f691f61d650d71349f496c3"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "28ce95a28cf04278999daed8a099a2c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "394d9c1390f1484f82d47899bbabbde9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a23ea6464e643d5ad642fd2ac9864b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40111ff076384a9297ead2f816aff1cb",
      "placeholder": "​",
      "style": "IPY_MODEL_967eb2d314f44aec89a0d1c6bed078f6",
      "value": "100%"
     }
    },
    "40111ff076384a9297ead2f816aff1cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57280342733a429fa2d327788d8c1a87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "767b54422b024a958e28adc0604c8326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b07dded0028443e86a65b02c9afcfdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bd2eef682944619ab06d381d2dc390d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8be477635be44e9f87dff8af41017553": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b07dded0028443e86a65b02c9afcfdf",
      "placeholder": "​",
      "style": "IPY_MODEL_57280342733a429fa2d327788d8c1a87",
      "value": "100%"
     }
    },
    "8e171faf256a481aafac35d0416f5001": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8bd2eef682944619ab06d381d2dc390d",
      "placeholder": "​",
      "style": "IPY_MODEL_767b54422b024a958e28adc0604c8326",
      "value": " 85.7M/85.7M [00:04&lt;00:00, 26.3MB/s]"
     }
    },
    "90c4a4ed8f9e4343b5ff7dc507342829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "967eb2d314f44aec89a0d1c6bed078f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b3f973175a44a96b28f01f3b2be5ee6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fe6e45e0c2b4542800ce9ff6251dc94",
      "max": 89843225,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90c4a4ed8f9e4343b5ff7dc507342829",
      "value": 89843225
     }
    },
    "9fe6e45e0c2b4542800ce9ff6251dc94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1453a0cf1f94168993482ffe76fa721": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3e566c81fe642479a5c29a01f96cedc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1047a5345a34e028d4ba5a2f38f2c31",
      "placeholder": "​",
      "style": "IPY_MODEL_a1453a0cf1f94168993482ffe76fa721",
      "value": " 91.2M/91.2M [00:04&lt;00:00, 26.4MB/s]"
     }
    },
    "a4539b483292426dadd6aa0d0f74ac38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8be477635be44e9f87dff8af41017553",
       "IPY_MODEL_ff6feac3822e4bcea52220ee2f00d8cd",
       "IPY_MODEL_a3e566c81fe642479a5c29a01f96cedc"
      ],
      "layout": "IPY_MODEL_28ce95a28cf04278999daed8a099a2c5"
     }
    },
    "b8d2defd6a4e4f56a96a9dab7718aac7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a23ea6464e643d5ad642fd2ac9864b6",
       "IPY_MODEL_9b3f973175a44a96b28f01f3b2be5ee6",
       "IPY_MODEL_8e171faf256a481aafac35d0416f5001"
      ],
      "layout": "IPY_MODEL_e523c3f102f14a6fa60cd5b74c4b48ce"
     }
    },
    "ced67228d3bd477bab466e8e1c4ae3ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e1047a5345a34e028d4ba5a2f38f2c31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e523c3f102f14a6fa60cd5b74c4b48ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff6feac3822e4bcea52220ee2f00d8cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_394d9c1390f1484f82d47899bbabbde9",
      "max": 95641761,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ced67228d3bd477bab466e8e1c4ae3ef",
      "value": 95641761
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
